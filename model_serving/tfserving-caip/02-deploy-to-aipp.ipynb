{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TF Serving with AI Platform Prediction Custom Containers (Beta)\n",
    "\n",
    "This notebook demonstrates how to deploy a TensorFlow 2.x model using AI Platform Prediction Custom Containers (Beta) and TensorFlow Serving.\n",
    "\n",
    "\n",
    "Although, this notebook uses the custom serving module developed in the `01-prepare-for-serving.ipynb` notebook, the discussed techniques can be applied to any TF 2.x model.\n",
    "\n",
    "For more information about the AI Platform Prediction Custom Containers feature refer to [TBD]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import google.auth\n",
    "\n",
    "from google.auth.credentials import Credentials\n",
    "from google.auth.transport.requests import AuthorizedSession\n",
    "\n",
    "from typing import List, Optional, Text, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n",
    "\n",
    "This notebook was tested on **AI Platform Notebooks** using the standard TF 2.2 image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the model store path\n",
    "\n",
    "Set the `SAVED_MODEL_PATH` to the GCS location of the `SavedModel` created in the `01-prepare-for-serving.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_MODEL_PATH = 'gs://mlops-dev-workspace/models/resnet_serving'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push the TF Serving container image to the local GCR\n",
    "\n",
    "A container image that is used with AI Platform Prediction must be in the GCP Container Registry. Retrieving the container from an external registry like Docker Hub is not supported. In this example we are using the standard TensorFlow Serving docker image. To make it available to AI Platform Prediction you will upload it to the current project's Container Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ , project_id = google.auth.default()\n",
    "\n",
    "cpu_image_name = 'gcr.io/{}/tensorflow_serving:latest-cpu'.format(project_id)\n",
    "gpu_image_name = 'gcr.io/{}/tensorflow_serving:latest-gpu'.format(project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest: Pulling from tensorflow/serving\n",
      "Digest: sha256:a94b7e3b0e825350675e83b0c2f2fc28f34be358c34e4126a1d828de899ec44f\n",
      "Status: Image is up to date for tensorflow/serving:latest\n",
      "docker.io/tensorflow/serving:latest\n",
      "latest-gpu: Pulling from tensorflow/serving\n",
      "Digest: sha256:9f2154baa458bf7b523d5f3c9f545056ed14d75ceac00742d1903d37d80393e9\n",
      "Status: Image is up to date for tensorflow/serving:latest-gpu\n",
      "docker.io/tensorflow/serving:latest-gpu\n"
     ]
    }
   ],
   "source": [
    "!docker pull tensorflow/serving:latest\n",
    "!docker pull tensorflow/serving:latest-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker tag tensorflow/serving:latest {cpu_image_name}\n",
    "!docker tag tensorflow/serving:latest-gpu {gpu_image_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [gcr.io/mlops-dev-env/tensorflow_serving]\n",
      "\n",
      "\u001b[1Bac716820: Preparing \n",
      "\u001b[1Bbd8c4bd3: Preparing \n",
      "\u001b[1Be785c230: Preparing \n",
      "\u001b[1Ba73fd165: Preparing \n",
      "\u001b[1Bf9a74649: Preparing \n",
      "\u001b[1Bda143c91: Preparing \n",
      "\u001b[1B287e1f04: Preparing \n",
      "\u001b[1B68776582: Layer already exists \u001b[4A\u001b[2K\u001b[1A\u001b[2Klatest-cpu: digest: sha256:a94b7e3b0e825350675e83b0c2f2fc28f34be358c34e4126a1d828de899ec44f size: 1989\n",
      "The push refers to repository [gcr.io/mlops-dev-env/tensorflow_serving]\n",
      "\n",
      "\u001b[1B41b4553f: Preparing \n",
      "\u001b[1B6ab262b7: Preparing \n",
      "\u001b[1Bfdb5f1f9: Preparing \n",
      "\u001b[1B64ade40f: Preparing \n",
      "\u001b[1B0889ee68: Preparing \n",
      "\u001b[1Bd332a58a: Preparing \n",
      "\u001b[1Bf11cbf29: Preparing \n",
      "\u001b[1Ba4b22186: Preparing \n",
      "\u001b[1Bafb09dc3: Preparing \n",
      "\u001b[1Bb5a53aac: Preparing \n",
      "\u001b[1Bc8e5063e: Preparing \n",
      "\u001b[1B7c529ced: Layer already exists \u001b[6A\u001b[2K\u001b[3A\u001b[2Klatest-gpu: digest: sha256:9f2154baa458bf7b523d5f3c9f545056ed14d75ceac00742d1903d37d80393e9 size: 2835\n"
     ]
    }
   ],
   "source": [
    "!docker push {cpu_image_name}\n",
    "!docker push {gpu_image_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying model versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an authorized session \n",
    "\n",
    "You will be using the AI Platform Prediction REST API to deploy a container. The API uses OAuth 2 for authentication. Instead of manually generating and maintaining OAuth tokens, you will use the `google.auth.transport.requests.AuthorizedSession` client that encapsulates the OAuth workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_endpoint = 'https://alpha-ml.googleapis.com'\n",
    "\n",
    "credentials, project_ = google.auth.default()\n",
    "authed_session = AuthorizedSession(credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all models in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models': [{'name': 'projects/mlops-dev-env/models/ResNet101',\n",
       "   'regions': ['us-central1'],\n",
       "   'etag': 'S7FgvSfwfUY='}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = f'{service_endpoint}/v1/projects/{project_id}/models/'\n",
    "\n",
    "response = authed_session.get(url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': {'code': 409,\n",
       "  'message': 'Field: model.name Error: A model with the same name already exists.',\n",
       "  'status': 'ALREADY_EXISTS',\n",
       "  'details': [{'@type': 'type.googleapis.com/google.rpc.BadRequest',\n",
       "    'fieldViolations': [{'field': 'model.name',\n",
       "      'description': 'A model with the same name already exists.'}]}]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'ResNet101'\n",
    "\n",
    "url = f'{service_endpoint}/v1/projects/{project_id}/models/'\n",
    "\n",
    "request_body = {\n",
    "    \"name\": model_name\n",
    "}\n",
    "\n",
    "response = authed_session.post(url, data=json.dumps(request_body))\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the model's info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'projects/mlops-dev-env/models/ResNet101',\n",
       " 'regions': ['us-central1'],\n",
       " 'etag': 'S7FgvSfwfUY='}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = f'{service_endpoint}/v1/projects/{project_id}/models/{model_name}'\n",
    "\n",
    "response = authed_session.get(url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model version\n",
    "\n",
    "When deploying a custom container to AI Platform Prediction you need to configure two groups of settings. The first group defines the configuration of the AI Platform Prediction service that hosts your container. For example, a node type, manual or autoscaling parameters, an accelerator configuration, etc. The second group are the settings specific to a given container. \n",
    "\n",
    "Refer to [TBD]() for a detailed discussion of the available service settings.\n",
    "\n",
    "There are three ways of passing configuration settings to a container:\n",
    "* the settings can be embedded in a custom container image\n",
    "* you can pass the settings as command line arguments, or \n",
    "* you can supply a configuration file. \n",
    "\n",
    "In the first method, the configuration settings are supplied  at the time the container container is built. The other two methods allow you to set the settings  at the deployment time. \n",
    "\n",
    "Some model servers commonly used in AI Platform Prediction custom containers, including TF Serving used in this notebook, also expose a management API that allows you to change configurations after the server has been deployed. Configuring the server through the management API is currently not supported due to the constraints of the REST interface exposed by AI Platform Prediction.\n",
    "\n",
    "\n",
    "Supplying configuration settings through a command line interface is straightforward. The AI Platform Prediction REST API utilizes JSON to encode requests and responses. You can provide the command line arguments as the `args` key of the JSON `container` object in the create model version request body.\n",
    "\n",
    "\n",
    "Passing a config file to a container hosted in AI Platform Prediction is a little bit trickier. The container runs in an isolated environment and does not have access to resources (including Cloud Storage) outside of this environment. To pass file based assets (including a config file) to the container you need to stage them in the GCS deployment location. The GCS deployment location - set through the `deployment_uri` field of the REST API request body - is copied to the isolated environment by the create model version request. The url to the location of the copy in the isolated environment is exposed through the `AIP_STORAGE_URI` environment variable. \n",
    "\n",
    "In the following example you will use both the command line arguments and the configuration file to configure the TF Serving model server. Most of the configurations will be passed as command line arguments. The [server side batching]()(https://www.tensorflow.org/tfx/serving/serving_config#batching_configuration) parameters will be passed as a config file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the config file with batching settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batching_config = '/tmp/batching.pbtxt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/batching.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {batching_config}\n",
    "\n",
    "max_batch_size { value: 128 }\n",
    "batch_timeout_micros { value: 150000 }\n",
    "max_enqueued_batches { value: 16 }\n",
    "num_batch_threads { value: 8 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy the batch config file to the staging location in GCS\n",
    "\n",
    "You are going to use the folder where the custom ResNet10 model was saved as the staging location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///tmp/batching.pbtxt [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  136.0 B/  136.0 B]                                                \n",
      "Operation completed over 1 objects/136.0 B.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp {batching_config} {SAVED_MODEL_PATH}/{batching_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "max_batch_size { value: 128 }\n",
      "batch_timeout_micros { value: 150000 }\n",
      "max_enqueued_batches { value: 16 }\n",
      "num_batch_threads { value: 8 }\n"
     ]
    }
   ],
   "source": [
    "!gsutil cat {SAVED_MODEL_PATH}/batching.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'projects/mlops-dev-env/operations/create_ResNet101_batching_150-1597681616585',\n",
       " 'metadata': {'@type': 'type.googleapis.com/google.cloud.ml.v1.OperationMetadata',\n",
       "  'createTime': '2020-08-17T16:26:57Z',\n",
       "  'operationType': 'CREATE_VERSION',\n",
       "  'modelName': 'projects/mlops-dev-env/models/ResNet101',\n",
       "  'version': {'name': 'projects/mlops-dev-env/models/ResNet101/versions/batching_150',\n",
       "   'deploymentUri': 'gs://mlops-dev-workspace/models/resnet_serving',\n",
       "   'createTime': '2020-08-17T16:26:56Z',\n",
       "   'etag': 'ZhHucsxrHMI=',\n",
       "   'machineType': 'n1-standard-8',\n",
       "   'acceleratorConfig': {'count': '1', 'type': 'NVIDIA_TESLA_P4'},\n",
       "   'container': {'image': 'gcr.io/mlops-dev-env/tensorflow_serving:latest-gpu',\n",
       "    'args': ['--rest_api_port=8080',\n",
       "     '--model_name=ResNet101',\n",
       "     '--model_base_path=$(AIP_STORAGE_URI)',\n",
       "     '--enable_batching',\n",
       "     '--batching_parameters_file=$(AIP_STORAGE_URI)/batching.pbtxt']},\n",
       "   'routes': {'predict': '/v1/models/ResNet101:predict',\n",
       "    'health': '/v1/models/ResNet101'}}}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version_name = 'batching_150'\n",
    "\n",
    "url = f'{service_endpoint}/v1/projects/{project_id}/models/{model_name}/versions'\n",
    "\n",
    "request_body = {\n",
    "    # Service settings\n",
    "    \"name\": version_name,\n",
    "    \"deployment_uri\": SAVED_MODEL_PATH,\n",
    "    \"machine_type\": 'n1-standard-8',\n",
    "    \"accelerator_config\": {\n",
    "        \"count\": 1,\n",
    "        \"type\": 'NVIDIA_TESLA_P4'},\n",
    "    \"routes\": {\n",
    "        \"predict\": f\"/v1/models/{model_name}:predict\",\n",
    "        \"health\": f\"/v1/models/{model_name}\"},\n",
    "    \n",
    "    # Container settings\n",
    "    \"container\": {\n",
    "        \"image\": gpu_image_name,\n",
    "        \"args\": [\n",
    "            \"--rest_api_port=8080\",\n",
    "            f\"--model_name={model_name}\",\n",
    "            \"--model_base_path=$(AIP_STORAGE_URI)\",\n",
    "            \"--enable_batching\",\n",
    "            \"--batching_parameters_file=$(AIP_STORAGE_URI)/batching.pbtxt\"]}\n",
    "}\n",
    "            \n",
    "response = authed_session.post(url, data=json.dumps(request_body))\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the deployment status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'projects/mlops-dev-env/models/ResNet101/versions/batching_150',\n",
       " 'deploymentUri': 'gs://mlops-dev-workspace/models/resnet_serving',\n",
       " 'createTime': '2020-08-17T16:26:56Z',\n",
       " 'state': 'CREATING',\n",
       " 'etag': '8O+3Tg7ULTM=',\n",
       " 'machineType': 'n1-standard-8',\n",
       " 'acceleratorConfig': {'count': '1', 'type': 'NVIDIA_TESLA_P4'},\n",
       " 'container': {'image': 'gcr.io/mlops-dev-env/tensorflow_serving:latest-gpu',\n",
       "  'args': ['--rest_api_port=8080',\n",
       "   '--model_name=ResNet101',\n",
       "   '--model_base_path=$(AIP_STORAGE_URI)',\n",
       "   '--enable_batching',\n",
       "   '--batching_parameters_file=$(AIP_STORAGE_URI)/batching.pbtxt']},\n",
       " 'routes': {'predict': '/v1/models/ResNet101:predict',\n",
       "  'health': '/v1/models/ResNet101'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = f'{service_endpoint}/v1/projects/{project_id}/models/{model_name}/versions/{version_name}'\n",
    "\n",
    "response = authed_session.get(url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the deployed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now run inference by invoking the TF Serving `Predict` API.\n",
    "\n",
    "Refer to the [TF Serving REST API Reference](https://www.tensorflow.org/tfx/serving/api_rest) for more information about the API format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'locust/locust-image/test_images'\n",
    "raw_images = [tf.io.read_file(os.path.join(image_folder, image_path)).numpy()\n",
    "         for image_path in os.listdir(image_folder)]\n",
    "\n",
    "encoded_images = [{'b64': base64.b64encode(image).decode('utf-8')} for image in raw_images]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call the `predict` endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature = 'serving_preprocess'\n",
    "\n",
    "client.call_predict(\n",
    "    project_id=project_id, \n",
    "    model_name=model_name, \n",
    "    version_name=version_name, \n",
    "    signature=signature,\n",
    "    instances=encoded_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete model version and model resources\n",
    "#### List model versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ResNet101'\n",
    "\n",
    "url = f'{service_endpoint}/v1/projects/{project_id}/models/{model_name}/versions'\n",
    "\n",
    "response = authed_session.get(url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete the specific version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_name = 'batching_150'\n",
    "\n",
    "url = f'{service_endpoint}/v1/projects/{project_id}/models/{model_name}/versions/{version_name}'\n",
    "\n",
    "response = authed_session.delete(url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'{service_endpoint}/v1/projects/{project_id}/models/{model_name}'\n",
    "\n",
    "response = authed_session.delete(url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Walk through the `aipp_deploy.ipynb` notebook to learn how to deploy the custom serving module created in this notebook to **AI Platform Prediction** using TF Serving container image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=-1>Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.</font>"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
